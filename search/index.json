[{"content":"\nYes, yes, yes, I know‚Ä¶ I don\u0026rsquo;t like graphical interfaces so sometimes I like to make some TUI tools, and sometimes they can be ugly too‚Ä¶\nI was tired of wasting time clicking each time on the web extension or on the client, of having to systematically enter my login details, of not having a quick access directly accessible in shortcuts, etc.\nSo, living in a terminal 90% of the time, I thought I\u0026rsquo;d do a quick script to put me out of my misery.\nI know, judge me, please yourself :\n1 2 3 4 # b # [‚Ä¶] bw list items --search \u0026#34;${1}\u0026#34; --session \u0026#34;${BW_SESSION}\u0026#34; | jq -j \u0026#39;.[] | [.name, .login.password]\u0026#39; | tr -d \u0026#39;\\n[\u0026#34;\u0026#39;| tr \u0026#39;]\u0026#39; \u0026#39;\\n\u0026#39;| tr \u0026#39;,\u0026#39; \u0026#39;:\u0026#39; | sed \u0026#39;s/^\\ \\ //g\u0026#39; | \u0026#34;${FZF}\u0026#34; --layout=reverse | tail -1 | tr -d \u0026#39;\u0026#34;\u0026#39; | cut -d \u0026#39;:\u0026#39; -f 2 | sed \u0026#39;s/^\\ \\ //g\u0026#39;| cat | xclip -se c \u0026gt; /dev/null 2\u0026gt; /dev/null # [‚Ä¶] I needed Bitwarden CLI tool bw, a way to copy in clipboard password xclip, some json parsing with jq and fzf (and fzf-tmux).\nSo dependencies are :\nbw (Bitwarden cli) xclip jq fzf fzf-tmux To install this quick and dirty script, you need to create a Bitwarden token, check documentation here. Then set variables in b script.\n1 2 3 4 # Fill these vars export BW_CLIENTID=\u0026#39;\u0026#39; export BW_CLIENTSECRET=\u0026#39;\u0026#39; export BW_PASSWORD=\u0026#39;\u0026#39; Finally move b file to /usr/local/bin.\n1 sudo mv b /usr/local/bin You can then :\nSearch for a password 1 2 # ex, search github password b github Sync vault (download new password from Bitwarden/Vaultwarden server): 1 b --sync And probably a lot more to come as I could be a lazy man.\nFull script :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash export BW_CLIENTID=\u0026#39;\u0026#39; export BW_CLIENTSECRET=\u0026#39;\u0026#39; export BW_PASSWORD=\u0026#39;\u0026#39; export LOCK=\u0026#34;$HOME/.bw.lock\u0026#34; # is unlock ? if test -f \u0026#34;${LOCK}\u0026#34;; then BW_SESSION=$(bw unlock --passwordenv BW_PASSWORD | grep export | cut -d \u0026#39;\u0026#34;\u0026#39; -f 2 | tr -d \u0026#39;\u0026#34;\u0026#39; \u0026amp;\u0026amp; bw sync) else bw login --apikey \u0026amp;\u0026amp; touch \u0026#34;${LOCK}\u0026#34; fi # is inside tmux ? if [ -z \u0026#34;${TMUX}\u0026#34; ] ; then FZF=\u0026#34;fzf\u0026#34; else FZF=\u0026#34;fzf-tmux\u0026#34; fi if [ \u0026#34;${1}\u0026#34; = \u0026#34;--sync\u0026#34; ]; then bw sync exit fi bw list items --search \u0026#34;${1}\u0026#34; --session \u0026#34;${BW_SESSION}\u0026#34; | jq -j \u0026#39;.[] | [.name, .login.password]\u0026#39; | tr -d \u0026#39;\\n[\u0026#34;\u0026#39;| tr \u0026#39;]\u0026#39; \u0026#39;\\n\u0026#39;| tr \u0026#39;,\u0026#39; \u0026#39;:\u0026#39; | sed \u0026#39;s/^\\ \\ //g\u0026#39; | \u0026#34;${FZF}\u0026#34; --layout=reverse | tail -1 | tr -d \u0026#39;\u0026#34;\u0026#39; | cut -d \u0026#39;:\u0026#39; -f 2 | sed \u0026#39;s/^\\ \\ //g\u0026#39;| cat | xclip -se c \u0026gt; /dev/null 2\u0026gt; /dev/null Sources are here.\n","date":"2023-08-09T13:10:42+02:00","permalink":"https://lebaron.sh/p/b-an-ugly-bitwarden-cli/","title":"B, an ugly Bitwarden CLI"},{"content":" Quick post on great Zellij tool I\u0026rsquo;ve been using for some weeks now in Tmux replacement.\nI\u0026rsquo;ve been missing to manage my sessions on multiplexer init while starting a new shell or terminal so I\u0026rsquo;ve ended creating a quick feature to manage it on startup.\nDependencies You need sk binary installed and in your $PATH and of course zellij.\nDemo Installation Add this block at the end of your $SHELLrc file (tested with BASH and ZSH) :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ZJ_SESSIONS=$(zellij list-sessions) NO_SESSIONS=$(echo \u0026#34;${ZJ_SESSIONS}\u0026#34; | wc -l) if [ \u0026#34;{$ZELLIJ}\u0026#34; ] \u0026amp;\u0026amp; [ -z \u0026#34;${ZELLIJ_SESSION_NAME}\u0026#34; ]; then echo -ne \u0026#34;Active Zellij sessions :\\n\u0026#34; for i in $(echo \u0026#34;${ZJ_SESSIONS}\u0026#34;); do echo -ne \u0026#34;*${i}\\n\u0026#34;; done echo -ne \u0026#39;\\n\u0026#39; read REPLY\\?\u0026#34;New zellij session [y/n] ? \u0026#34; if [ \u0026#34;${REPLY}\u0026#34; = \u0026#34;y\u0026#34; ]; then read SESS\\?\u0026#34;Session name : \u0026#34; zellij --layout compact attach -c \u0026#34;${SESS}\u0026#34; else if [ \u0026#34;${NO_SESSIONS}\u0026#34; -ge 1 ]; then zellij --layout compact attach \\ \u0026#34;$(echo \u0026#34;${ZJ_SESSIONS}\u0026#34; | sk)\u0026#34; else fi fi fi Contributing Feel free to fork and contribute.\nSources are here.\n","date":"2022-07-12T12:22:42+02:00","permalink":"https://lebaron.sh/p/manage-your-zellij-sessions/","title":"Manage your Zellij sessions"},{"content":"\nWhy Features Preview Templating Usage Parsing Arguments Shell Configurations Trap Error and Exit Display Loader Call Loader Scrip Library Integration Conclusion Why I\u0026rsquo;ve been recently doing some old shell scripts to quickly automatize and distribute actions that were made manually, I\u0026rsquo;ll write a post on it later. For now I\u0026rsquo;d like to share with you how I\u0026rsquo;ve been coding a 100% shell loader library to use it in my scripts.\nI\u0026rsquo;ve been looking on the net for existing library, I\u0026rsquo;ve found some interesting Github projects but it doesn\u0026rsquo;t appear to be \u0026ldquo;library\u0026rdquo; ready. Furthermore, it was mostly old basic ASCII templates and I¬†wanted to give a try to modern loaders and spinners on shell.\nSo I ended create shloader.\nFeatures shloader has nice features such as :\nüòç emoji support\nüí™ loader support\nüòé dynamic message on load step\n‚ÑπÔ∏è message on step ending\nüé® multiple loading templates\nüëå light and easy to use on existing scripts\nPreview Templating First of all, we need to create templates, one for each loaders that will be displayed. I\u0026rsquo;ve decided to use shell array so we can work on iteration later on customs functions.\nSo I ended with something like :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # EMOJIS emoji_hour=( 0.08 \u0026#39;üïõ\u0026#39; \u0026#39;üïê\u0026#39; \u0026#39;üïë\u0026#39; \u0026#39;üïí\u0026#39; \u0026#39;üïì\u0026#39; \u0026#39;üïî\u0026#39; \u0026#39;üïï\u0026#39; \u0026#39;üïñ\u0026#39; \u0026#39;üïó\u0026#39; \u0026#39;üïò\u0026#39; \u0026#39;üïô\u0026#39; \u0026#39;üïö\u0026#39;) emoji_face=( 0.08 \u0026#39;üòê\u0026#39; \u0026#39;üòÄ\u0026#39; \u0026#39;üòç\u0026#39; \u0026#39;üôÑ\u0026#39; \u0026#39;üòí\u0026#39; \u0026#39;üò®\u0026#39; \u0026#39;üò°\u0026#39;) emoji_earth=( 0.1 üåç üåé üåè ) emoji_moon=( 0.08 üåë üåí üåì üåî üåï üåñ üåó üåò ) emoji_orange_pulse=( 0.1 üî∏ üî∂ üü† üü† üî∂ ) emoji_blue_pulse=( 0.1 üîπ üî∑ üîµ üîµ üî∑ ) emoji_blink=( 0.06 üòê üòê üòê üòê üòê üòê üòê üòê üòê üòë ) emoji_camera=( 0.05 üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∑ üì∏ üì∑ üì∏ ) emoji_sick=( 0.2 ü§¢ ü§¢ ü§Æ ) emoji_monkey=( 0.2 üôâ üôà üôä üôà ) emoji_bomb=( 0.2 \u0026#39;üí£ \u0026#39; \u0026#39; üí£ \u0026#39; \u0026#39; üí£ \u0026#39; \u0026#39; üí£\u0026#39; \u0026#39; üí£\u0026#39; \u0026#39; üí£\u0026#39; \u0026#39; üí£\u0026#39; \u0026#39; üí£\u0026#39; \u0026#39; üí•\u0026#39; \u0026#39; \u0026#39; \u0026#39; \u0026#39; ) # ASCII ball=( 0.2 \u0026#39;(‚óè)\u0026#39; \u0026#39;(‚ö¨)\u0026#39;) arrow=( 0.06 \u0026#39;‚Üë\u0026#39; \u0026#39;‚Üó\u0026#39; \u0026#39;‚Üí\u0026#39; \u0026#39;‚Üò\u0026#39; \u0026#39;‚Üì\u0026#39; \u0026#39;‚Üô\u0026#39; \u0026#39;‚Üê\u0026#39; \u0026#39;‚Üñ\u0026#39;) cym=( 0.1 \u0026#39;‚äè\u0026#39; \u0026#39;‚äì\u0026#39; \u0026#39;‚äê\u0026#39; \u0026#39;‚äî\u0026#39;) x_plus=( 0.08 \u0026#39;√ó\u0026#39; \u0026#39;+\u0026#39;) line=( 0.08 \u0026#39;‚ò∞\u0026#39; \u0026#39;‚ò±\u0026#39; \u0026#39;‚ò≥\u0026#39; \u0026#39;‚ò∑\u0026#39; \u0026#39;‚ò∂\u0026#39; \u0026#39;‚ò¥\u0026#39;) ball_wave=( 0.1 \u0026#39;ìÉâìÉâìÉâ\u0026#39; \u0026#39;ìÉâìÉâ‚àò\u0026#39; \u0026#39;ìÉâ‚àò¬∞\u0026#39; \u0026#39;‚àò¬∞‚àò\u0026#39; \u0026#39;¬∞‚àòìÉâ\u0026#39; \u0026#39;‚àòìÉâìÉâ\u0026#39;) old=( 0.07 \u0026#39;‚Äî\u0026#39; \u0026#34;\\\\\u0026#34; \u0026#39;|\u0026#39; \u0026#39;/\u0026#39; ) dots=( 0.04 \u0026#39;‚£æ\u0026#39; \u0026#39;‚£Ω\u0026#39; \u0026#39;‚£ª\u0026#39; \u0026#39;‚¢ø\u0026#39; \u0026#39;‚°ø\u0026#39; \u0026#39;‚£ü\u0026#39; \u0026#39;‚£Ø\u0026#39; \u0026#39;‚£∑\u0026#39; ) dots2=( 0.04 \u0026#39;‚†ã\u0026#39; \u0026#39;‚†ô\u0026#39; \u0026#39;‚†π\u0026#39; \u0026#39;‚†∏\u0026#39; \u0026#39;‚†º\u0026#39; \u0026#39;‚†¥\u0026#39; \u0026#39;‚†¶\u0026#39; \u0026#39;‚†ß\u0026#39; \u0026#39;‚†á\u0026#39; \u0026#39;‚†è\u0026#39; ) dots3=( 0.04 \u0026#39;‚†ã\u0026#39; \u0026#39;‚†ô\u0026#39; \u0026#39;‚†ö\u0026#39; \u0026#39;‚†û\u0026#39; \u0026#39;‚†ñ\u0026#39; \u0026#39;‚†¶\u0026#39; \u0026#39;‚†¥\u0026#39; \u0026#39;‚†≤\u0026#39; \u0026#39;‚†≥\u0026#39; \u0026#39;‚†ì\u0026#39; ) dots4=( 0.04 \u0026#39;‚†Ñ\u0026#39; \u0026#39;‚†Ü\u0026#39; \u0026#39;‚†á\u0026#39; \u0026#39;‚†ã\u0026#39; \u0026#39;‚†ô\u0026#39; \u0026#39;‚†∏\u0026#39; \u0026#39;‚†∞\u0026#39; \u0026#39;‚††\u0026#39; \u0026#39;‚†∞\u0026#39; \u0026#39;‚†∏\u0026#39; \u0026#39;‚†ô\u0026#39; \u0026#39;‚†ã\u0026#39; \u0026#39;‚†á\u0026#39; \u0026#39;‚†Ü\u0026#39; ) dots5=( 0.04 \u0026#39;‚†ã\u0026#39; \u0026#39;‚†ô\u0026#39; \u0026#39;‚†ö\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†Ç\u0026#39; \u0026#39;‚†Ç\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†≤\u0026#39; \u0026#39;‚†¥\u0026#39; \u0026#39;‚†¶\u0026#39; \u0026#39;‚†ñ\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†ê\u0026#39; \u0026#39;‚†ê\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†ì\u0026#39; \u0026#39;‚†ã\u0026#39; ) dots6=( 0.04 \u0026#39;‚†Å\u0026#39; \u0026#39;‚†â\u0026#39; \u0026#39;‚†ô\u0026#39; \u0026#39;‚†ö\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†Ç\u0026#39; \u0026#39;‚†Ç\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†≤\u0026#39; \u0026#39;‚†¥\u0026#39; \u0026#39;‚†§\u0026#39; \u0026#39;‚†Ñ\u0026#39; \u0026#39;‚†Ñ\u0026#39; \u0026#39;‚†§\u0026#39; \u0026#39;‚†¥\u0026#39; \u0026#39;‚†≤\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†Ç\u0026#39; \u0026#39;‚†Ç\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†ö\u0026#39; \u0026#39;‚†ô\u0026#39; \u0026#39;‚†â\u0026#39; \u0026#39;‚†Å\u0026#39; ) dots7=( 0.04 \u0026#39;‚†à\u0026#39; \u0026#39;‚†â\u0026#39; \u0026#39;‚†ã\u0026#39; \u0026#39;‚†ì\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†ê\u0026#39; \u0026#39;‚†ê\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†ñ\u0026#39; \u0026#39;‚†¶\u0026#39; \u0026#39;‚†§\u0026#39; \u0026#39;‚††\u0026#39; \u0026#39;‚††\u0026#39; \u0026#39;‚†§\u0026#39; \u0026#39;‚†¶\u0026#39; \u0026#39;‚†ñ\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†ê\u0026#39; \u0026#39;‚†ê\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†ì\u0026#39; \u0026#39;‚†ã\u0026#39; \u0026#39;‚†â\u0026#39; \u0026#39;‚†à\u0026#39; ) dots8=( 0.04 \u0026#39;‚†Å\u0026#39; \u0026#39;‚†Å\u0026#39; \u0026#39;‚†â\u0026#39; \u0026#39;‚†ô\u0026#39; \u0026#39;‚†ö\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†Ç\u0026#39; \u0026#39;‚†Ç\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†≤\u0026#39; \u0026#39;‚†¥\u0026#39; \u0026#39;‚†§\u0026#39; \u0026#39;‚†Ñ\u0026#39; \u0026#39;‚†Ñ\u0026#39; \u0026#39;‚†§\u0026#39; \u0026#39;‚††\u0026#39; \u0026#39;‚††\u0026#39; \u0026#39;‚†§\u0026#39; \u0026#39;‚†¶\u0026#39; \u0026#39;‚†ñ\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†ê\u0026#39; \u0026#39;‚†ê\u0026#39; \u0026#39;‚†í\u0026#39; \u0026#39;‚†ì\u0026#39; \u0026#39;‚†ã\u0026#39; \u0026#39;‚†â\u0026#39; \u0026#39;‚†à\u0026#39; \u0026#39;‚†à\u0026#39; ) dots9=( 0.04 \u0026#39;‚¢π\u0026#39; \u0026#39;‚¢∫\u0026#39; \u0026#39;‚¢º\u0026#39; \u0026#39;‚£∏\u0026#39; \u0026#39;‚£á\u0026#39; \u0026#39;‚°ß\u0026#39; \u0026#39;‚°ó\u0026#39; \u0026#39;‚°è\u0026#39; ) dots10=( 0.04 \u0026#39;‚¢Ñ\u0026#39; \u0026#39;‚¢Ç\u0026#39; \u0026#39;‚¢Å\u0026#39; \u0026#39;‚°Å\u0026#39; \u0026#39;‚°à\u0026#39; \u0026#39;‚°ê\u0026#39; \u0026#39;‚°†\u0026#39; ) dots11=( 0.04 \u0026#39;‚†Å\u0026#39; \u0026#39;‚†Ç\u0026#39; \u0026#39;‚†Ñ\u0026#39; \u0026#39;‚°Ä\u0026#39; \u0026#39;‚¢Ä\u0026#39; \u0026#39;‚††\u0026#39; \u0026#39;‚†ê\u0026#39; \u0026#39;‚†à\u0026#39; ) Array composition is divided in two parts, the first one is a time interval in seconds and the second one is the loader frame transition.\nUsage I wanted library to be used as commands can be, by passing argument through options.\nFirst, let\u0026rsquo;s work on default usage function to display to user.\n1 2 3 4 5 6 7 8 9 10 usage() { cat \u0026lt;\u0026lt;EOF Available options: -h, --help \u0026lt;OPTIONAL\u0026gt; Print this help and exit -l, --loader \u0026lt;OPTIONAL\u0026gt; Chose loader to display -m, --message \u0026lt;OPTIONAL\u0026gt; Text to display while loading -e, --ending \u0026lt;OPTIONAL\u0026gt; Text to display when finishing EOF exit 0 } I wanted to define argument options as bellow :\nDisplay help 1 2 # e.g shloader -h Parameter Type Description -h --help none Optional. Show help usage Chose loader 1 2 # e.g shloader -l arrow Parameter Type Description -l --loader string Optional. Chose loader template Display info on loading 1 2 # e.g shloader -m \u0026#34;my loading message\u0026#34; Parameter Type Description -m --message string Optional. Show a text message while displaying loader Display info on ending 1 2 # e.g shloader -e \u0026#34;\\u2728 all done\u0026#34; Parameter Type Description -e --end string Optional. Show an end text message when loader ends Parsing Arguments In order to work on user input, it is necessary to read option content so we can modular library execution.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 shloader() { loader=\u0026#39;\u0026#39; message=\u0026#39;\u0026#39; ending=\u0026#39;\u0026#39; while :; do case \u0026#34;${1-}\u0026#34; in -h | --help) usage;; -l | --loader) loader=\u0026#34;${2-}\u0026#34; shift ;; -m | --message) message=\u0026#34;${2-}\u0026#34; shift ;; -e | --ending) ending=\u0026#34;${2-}\u0026#34; shift ;; -?*) die \u0026#34;Unknown option: $1\u0026#34; ;; *) break ;; esac shift done [‚Ä¶] } You might notice I\u0026rsquo;ve placed it directly in the main function so it will be the first element to be run.\nIf user don\u0026rsquo;t specify loader, we should display one as default, let\u0026rsquo;s say dots one.\n1 2 3 4 5 6 7 8 9 10 # shloader func [‚Ä¶] if [[ -z \u0026#34;${loader}\u0026#34; ]] ; then loader=dots[@] else loader=$loader[@] fi [‚Ä¶] One last thing here, I use a little custom die function to exit properly if option is unknown.\n1 2 3 4 die() { local code=${2-1} exit \u0026#34;$code\u0026#34; } Shell Configurations We will make some quick configuration on the top header script under shebang.\n1 2 3 4 5 6 #!/bin/bash # https://github.com/lebaronlebaron.shloader # me@lebaron.sh set -Eeuo pipefail trap end_shloader SIGINT SIGTERM ERR EXIT RETURN tput civis First, set -Eeuo pipefail so we can exit execution if one of the commands in the pipe fails.\nThen, trap end_shloader SIGINT SIGTERM ERR EXIT RETURN so we can call a custom function to clean our execution on exiting, stopping, errors‚Ä¶\nFinaly, tput civis is used to hide cursor.\nTrap Error and Exit Trap will call a custom function, but how does it work ?\n1 2 3 4 5 6 7 end_shloader() { kill \u0026#34;${shloader_pid}\u0026#34; \u0026amp;\u0026gt;/dev/null tput cnorm if [[ \u0026#34;${ending}\u0026#34; ]]; then printf \u0026#34;\\r${ending}\u0026#34;; echo fi } Right here I ensure first to kill loader\u0026rsquo;s PID and restore cursor thanks to tput cnorm.\nDisplay Loader Now we can interact with shell output to generate loading animations.\nLet\u0026rsquo;s create a new function play_shloader() :\n1 2 3 4 5 6 7 8 play_shloader() { while true ; do for frame in \u0026#34;${loader[@]}\u0026#34; ; do printf \u0026#34;\\r%s\u0026#34; \u0026#34;${frame} ${message}\u0026#34; sleep \u0026#34;${speed}\u0026#34; done done } Here we assume we have loader array we can display during time duration defined in speed variable (we will see it after).\nCall Loader What about now ? We have all bricks to make great loaders, let\u0026rsquo;s put them in work together !\n1 2 3 4 5 6 7 8 9 10 11 12 # shloader function [‚Ä¶] loader=( ${!loader} ) speed=\u0026#34;${loader[0]}\u0026#34; unset \u0026#34;loader[0]\u0026#34; tput civis play_shloader \u0026amp; shloader_pid=\u0026#34;${!}\u0026#34; [‚Ä¶] What is done here ?\nWe first print loader array content we save in loader variable. We know first element in array is time duration, so we save it in speed variable.\nNow we have split array, we can remove time duration as it is now under speed variable. We then hide cursor, call our play_shloader function and save PID.\nYou can find the Full Library Code\nScript Library Integration Nothing hard here !\n1 2 3 4 5 6 7 8 source ./lib/shloader.sh shloader -l emoji_hour -m \u0026#34;Testing\u0026#34; -e \u0026#34;‚ú® All good !\u0026#34; sleep 2 # remove it in your code # ‚Ä¶ your logic goes here end_shloader With more details :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 !/bin/bash # if you want to try just add this block code in your code source ./lib/shloader.sh # you can chose (see more in lib/loader.sh): # ball, arrow, cym, x_plus, line, ball_wave, npm and old. # you can specify message to display during loading # and message to display after your code finished # eg with npm style # notice end message -e use unicode emoji to display # this is for better terminal support # \\u2728 == ‚ú® but you can use emoji if your settings support it ! shloader -l emoji_face -m \u0026#34;Testing\u0026#34; -e \u0026#34;\\u2728 All good !\u0026#34; sleep 2 # remove it in your code # ‚Ä¶ your logic goes here # if you want to hide some output from loader # don\u0026#39;t forget to redirect your STD* # # eg : # STDOUT # my_cmd 1\u0026gt; /dev/null # STDERR # my_cmd 2\u0026gt; /dev/null # BOTH # my_cmd \u0026amp;\u0026gt; /dev/null # stop loader end_shloader Conclusion As all things useless‚Ä¶ it may become mandatory üòÇ.\n","date":"2022-06-25T18:03:17+02:00","permalink":"https://lebaron.sh/p/shloader-a-modern-shell-loader/","title":"Shloader - A Modern Shell Loader"},{"content":"\nThe Old Way Direnv What is direnv and how it works direnv installation Shell configuration How Python works with direnv Pyenv Installation and Configuration Use pyenv with direnv Poetry What is Poetry Poetry installation Shell configuration New project Existing project Poetry usage Link Poetry with direnv GPG Generate a key Get key ID Pass Pass installation Pass usage Tomb Tomb installation Tomb usage Secrets with Direnv and Pass Direnv advanced configurations Check for commands dependencies Third-party binaries Third-party configs Final template Conclusion Disclaimer: I\u0026rsquo;m not saying virtualenv methods are bad.\nThe Old way For years now, I\u0026rsquo;ve been using Python for a lot of different projects. My setup environment was quite simple, use virtualenv and work locally.\nBut I\u0026rsquo;ve faced few pain points by doing so. I was often forgetting to source venv/bin/activate, meaning I pip install my-package on my global system.\nI also had bad secrets management by setting them not encrypted in a Shell script loaded at my program start.\nAs bad as:\n1 2 3 4 5 export DB_NAME=\u0026#34;db_name\u0026#34; export DB_USER=\u0026#34;user\u0026#34; export DB_PASS=\u0026#34;pass\u0026#34; export DB_HOST=172.17.0.2 export DB_PORT=3306 Even if some great code editors can handle python interpreters in virtual environment, such as Vscode, I felt like my workflow automation was not as complete as it could be.\nDirenv How to ensure not forgetting to load a virtual environment and install package globally while working on our project ?\nWhat is direnv and how it works From official documentation\ndirenv is an extension for your shell. It augments existing shells with a new feature that can load and unload environment variables depending on the current directory. Before each prompt, direnv checks for the existence of a .envrc file (and optionally a .env file) in the current and parent directories. If the file exists (and is authorized), it is loaded into a bash sub-shell and all exported variables are then captured by direnv and then made available to the current shell.\nIt supports hooks for all the common shells like bash, zsh, tcsh and fish. This allows project-specific environment variables without cluttering the ~/.profile file.\nBecause direnv is compiled into a single static executable, it is fast enough to be unnoticeable on each prompt. It is also language-agnostic and can be used to build solutions similar to rbenv, pyenv and phpenv.\ndirenv installation direnv is accessible through packages in almost all distributions.\nPackage installation If you want a global system installation :\n1 Œª ~/ sudo apt-get install direnv Manual installation If you want custom installation, take a look on this script hosted by direnv official documentation.\n1 Œª ~/ curl -sfL https://direnv.net/install.sh | bash Shell configuration Once direnv is installed you need to configure your $SHELL in order to hook it with your default shell. It supports bash, zsh, fish, tcsh and elvish.\ne.g with zsh\n1 2 Œª ~/ echo \u0026#39;eval \u0026#34;$(direnv hook zsh)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc Œª ~/ source ~/.zshrc How python works with direnv We can load a Python virtual environment thanks to direnv, to do so we need to specify layout command in a .envrc file located at our root project.\nLet\u0026rsquo;s create our project directory first.\n1 Œª ~/ mkdir project \u0026amp;\u0026amp; cd project Creat our .envrc file.\n1 Œª ~/project/ echo \u0026#39;layout python3\u0026#39; \u0026gt; .envrc You will notice an error message like :\n1 direnv: error project/.envrc is blocked. Run `direnv allow` to approve its content This is a security way to block default content in your file. You can allow it thanks to :\n1 2 3 Œª ~/project/ direnv allow direnv: loading ~/project/.envrc direnv: export +VIRTUAL_ENV ~PATH Output inform us that virtual environment was automatically created for us. You won\u0026rsquo;t see any prompt modification (as virtualenv does while source venv/bin/activate) it\u0026rsquo;s normal.\nQuick check of our Python binary with :\n1 2 Œª ~/project/ which python project/.direnv/python-3.10.4/bin/python You can then work as you normally do through venv by installing your python dependencies.\n1 2 3 4 5 # install a single package Œª ~/project/ pip install django # or install your project dependencies Œª ~/project/ pip install -r requirements.txt Note: we will no longer use pip but poetry, see Poetry section on this post for more information.\nJust to be clear, direnv will automatically load your virtual environement when you move in your project directory. As soon as you will move out, environment will also be automatically deactivated.\n1 2 Œª ~/project/ cd direnv: unloading That\u0026rsquo;s could be enough for common use if you don\u0026rsquo;t need a specific Python version other than one installed on your system, in some cases you will want to work with specific versions, that\u0026rsquo;s why we need pyenv.\nPyenv pyenv lets you easily switch between multiple versions of Python. It\u0026rsquo;s simple, unobtrusive, and follows the UNIX tradition of single-purpose tools that do one thing well. It allows you to change Python version for each project and it supported by direnv since 2.21.0.\nInstallation and Configuration Get pyenv :\n1 Œª ~/ curl -L https://pyenv.run | bash Configure your $SHELL.\ne.g with zsh\n1 2 3 Œª ~/ echo \u0026#39;export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc Œª ~/ echo \u0026#39;command -v pyenv \u0026gt;/dev/null || export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc Œª ~/ echo \u0026#39;eval \u0026#34;$(pyenv init -)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc Check installation:\n1 2 Œª ~/ pyenv --version pyenv 2.3.1 And install latest Python version at this date 3.10.5:\n1 Œª ~/ pyenv install 3.10.5 Use pyenv with direnv Now pyenv is installed, let\u0026rsquo;s interface it with our project and direnv.\n1 2 3 Œª ~/ cd project/ Œª ~/ echo \u0026#39;layout pyenv 3.10.5\u0026#39; \u0026gt; .envrc Œª ~/ direnv allow Check our Python version and interpreter :\n1 2 3 Œª ~/project/ python --version \u0026amp;\u0026amp; which python Python 3.10.5 ~/project/.direnv/python-3.10.5/bin/python That\u0026rsquo;s it.\nPoetry What is Poetry Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you.\nPoetry will in fact replace pip usage and provide several useful advantages such as :\none configuration file for all dependencies and their configs can create and manage virtual environments \u0026lt;\u0026ndash; True, but we manage with direnv automatically resolves dependencies of installed plugins Poetry installation Poetry documentation provides installation guidelines and has a part for osx/linux/bashonwindows distributions :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Œª ~/ curl -sSL https://install.python-poetry.org | python3 - Retrieving Poetry metadata # Welcome to Poetry! This will download and install the latest version of Poetry, a dependency and package manager for Python. It will add the `poetry` command to Poetry\u0026#39;s bin directory, located at: $HOME/.local/bin You can uninstall at any time by executing this script with the --uninstall option, and these changes will be reverted. Installing Poetry (1.1.13): Done Poetry (1.1.13) is installed now. Great! You can test that everything is set up by executing: `poetry --version` You can check your installation and poetry version :\n1 2 Œª ~/ poetry --version Poetry version 1.1.13 Poetry shell Configuration You can enable tab completion for your shell. This documentation has guide for each shell.\ne.g with ZSH and Oh-my-zsh\n1 2 Œª ~/ mkdir $ZSH_CUSTOM/plugins/poetry poetry completions zsh \u0026gt; $ZSH_CUSTOM/plugins/poetry/_poetry For oh-my-zsh, you must then enable poetry in your ~/.zshrc plugins\n1 2 3 4 plugins( poetry ... ) Poetry usage From poetry documentation, you can use it for new project and|or existing one.\nNew project 1 Œª ~/ poetry new poetry-demo This will create the poetry-demo directory with the following content:\n1 2 3 4 5 6 7 poetry-demo ‚îú‚îÄ‚îÄ pyproject.toml ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ poetry_demo ‚îÇ ‚îî‚îÄ‚îÄ __init__.py ‚îî‚îÄ‚îÄ tests ‚îî‚îÄ‚îÄ __init__.py Existing project Instead of creating a new project, Poetry can be used to ‚Äòinitialise‚Äô a pre-populated directory. To interactively create a pyproject.toml file in directory pre-existing-project :\n1 2 Œª ~/ cd project Œª ~/project/ poetry init Add dependecies As simple as :\n1 2 3 4 5 6 7 8 9 10 11 12 13 Œª ~/project/ poetry add django Using version ^4.0.5 for Django Updating dependencies Resolving dependencies... (0.9s) Writing lock file Package operations: 3 installs, 0 updates, 0 removals ‚Ä¢ Installing asgiref (3.5.2) ‚Ä¢ Installing sqlparse (0.4.2) ‚Ä¢ Installing django (4.0.5) It will automatically find a suitable version constraint and install the package and sub-dependencies.\nFind details on your toml config file :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Œª ~/project/ cat pyproject.toml [tool.poetry] name = \u0026#34;project\u0026#34; version = \u0026#34;0.1.0\u0026#34; description = \u0026#34;\u0026#34; authors = [\u0026#34;Corentin Deret \u0026lt;corentin.deret@payfit.com\u0026gt;\u0026#34;] [tool.poetry.dependencies] python = \u0026#34;^3.10\u0026#34; Django = \u0026#34;^4.0.5\u0026#34; [tool.poetry.dev-dependencies] [build-system] requires = [\u0026#34;poetry-core\u0026gt;=1.0.0\u0026#34;] build-backend = \u0026#34;poetry.core.masonry.api\u0026#34; Link Poetry with direnv From poetry documentation :\nBy default, poetry creates a virtual environment in {cache-dir}/virtualenvs ({cache-dir}\\virtualenvs on Windows). You can change the cache-dir value by editing the poetry config. Additionally, you can use the virtualenvs.in-project configuration variable to create virtual environment within your project directory.\nWhat we want here, is to tell poetry not to configure its own environment but to use our previous direnv configuration.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # .envrc layout pyenv 3.10.5 # POETRY if [[ ! -f pyproject.toml ]]; then log_status \u0026#39;No pyproject.toml found. Will initialize poetry in no-interactive mode\u0026#39; poetry init -n -q poetry run pip install -U pip wheel setuptools fi poetry run echo \u0026gt;\u0026gt; /dev/null local VENV=$(dirname $(poetry run which python)) export VIRTUAL_ENV=$(echo \u0026#34;$VENV\u0026#34; | rev | cut -d\u0026#39;/\u0026#39; -f2- | rev) export POETRY_ACTIVE=1 PATH_add \u0026#34;$VENV\u0026#34; if [ ! -L .venv ]; then ln -ns $VIRTUAL_ENV .venv fi Let\u0026rsquo;s allow our direnv and check that we are using our correct environment.\nFor our example with django, we should be able to retrieve our dependencie in .direnv folder.\n1 2 3 4 Œª ~/project/ find .direnv -name \u0026#39;django\u0026#39; .direnv/python-3.10.5/lib/python3.10/site-packages/django .direnv/python-3.10.5/lib/python3.10/site-packages/django/forms/jinja2/django .direnv/python-3.10.5/lib/python3.10/site-packages/django/forms/templates/django GPG Remember about managing secrets ? Now that we have our virtual environements set, how can we manage to work with secrets without write them directly in our project/repository ?\nBefore we can use a password manager such as pass we need a gpg key id.\nGenerate a key Let\u0026rsquo;s first generete one.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 Œª ~/ gpg --full-generate-key Please select what kind of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (sign only) (4) RSA (sign only) (14) Existing key from card Your selection? 1 RSA keys may be between 1024 and 4096 bits long. What keysize do you want? (3072) Requested keysize is 3072 bits Please specify how long the key should be valid. 0 = key does not expire \u0026lt;n\u0026gt; = key expires in n days \u0026lt;n\u0026gt;w = key expires in n weeks \u0026lt;n\u0026gt;m = key expires in n months \u0026lt;n\u0026gt;y = key expires in n years Key is valid for? (0) Key does not expire at all Is this correct? (y/N) y Real name: Kader Ovski Email address: me@lebaron.sh Comment: You selected this USER-ID: \u0026#34;Kader Ovski \u0026lt;me@lebaron.sh\u0026gt;\u0026#34; Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy. gpg: key 0854057891EFB8F0 marked as ultimately trusted pub rsa3072 2022-06-10 [SC] DC3EC748A8D97169F47C16690854057891EFB8F0 uid Kader Ovski \u0026lt;me@lebaron.sh\u0026gt; sub rsa3072 2022-06-10 [E] Get key ID We can then list our new key :\n1 2 3 4 5 6 7 8 9 10 Œª ~/ gpg --list-keys gpg: checking the trustdb gpg: marginals needed: 3 completes needed: 1 trust model: pgp gpg: depth: 0 valid: 1 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 1u $HOME/.gnupg/pubring.kbx ------------------------ pub rsa3072 2022-06-10 [SC] DC3EC748A8D97169F47C16690854057891EFB8F0 uid [ultimate] Kader Ovski \u0026lt;me@lebaron.sh\u0026gt; sub rsa3072 2022-06-10 [E] Pass pass is the standard unix password manager.\nWith pass, each password lives inside of a gpg encrypted file whose filename is the title of the website or resource that requires the password. These encrypted files may be organized into meaningful folder hierarchies, copied from computer to computer, and, in general, manipulated using standard command line file management utilities.\nPass installation pass is available in almost all distribution and can be installed as follow:\n1 Œª ~/ sudo apt-get install pass Pass usage Now that we have pass installed on our system and a valid gpg_key_id we can init our password manager with :\n1 2 3 Œª ~/ pass init DC3EC748A8D97169F47C16690854057891EFB8F0 mkdir: created directory \u0026#39;$HOME/.password-store/\u0026#39; Password store initialized for DC3EC748A8D97169F47C16690854057891EFB8F0 Let\u0026rsquo;s insert our first password :\n1 2 3 4 5 Œª ~/ pass insert lebaron.sh/blog/some_secret mkdir: created directory \u0026#39;$HOME/.password-store/lebaron.sh\u0026#39; mkdir: created directory \u0026#39;$HOME/.password-store/lebaron.sh/blog\u0026#39; Enter password for lebaron.sh/blog/some_secret: Retype password for lebaron.sh/blog/some_secret: Our password is now store in our pass :\n1 2 3 4 5 Œª ~/ pass Password Store `-- lebaron.sh `-- blog `-- some_secret You can get your secret by typing and entering your gpg secret passphrase :\n1 2 Œª ~/ pass show lebaron.sh/blog/some_secret my_secret Tomb Due to the structure of pass, file and directory names are not encrypted in the password store. pass-tomb provides a convenient solution to put your password store in a Tomb and then keep your password tree encrypted when you are not using it.\nIt uses the same GPG key to encrypt passwords and tomb, therefore you don\u0026rsquo;t need to manage more key or secret. Moreover, you can ask pass-tomb to automatically close your store after a given time.\nTomb installation You can use package installation on your distribution :\n1 Œª ~/ sudo apt install pass-extension-tomb Tomb usage Create a new password tomb:\ne.g\n1 2 3 4 5 6 7 \u0026gt; pass tomb DC3EC748A8D97169F47C16690854057891EFB8F0 (*) Your password tomb has been created and opened in ~/.password-store. (*) Password store initialized for DC3EC748A8D97169F47C16690854057891EFB8F0 . Your tomb is: ~/.password.tomb . Your tomb key is: ~/.password.key.tomb . You can now use pass as usual. . When finished, close the password tomb using \u0026#39;pass close\u0026#39;. You can now use the best part of pass-tomb:\nopen your tomb 1 2 3 4 Œª ~/ pass open (*) Your password tomb has been opened in $HOME/.password-store/. . You can now use pass as usual. . When finished, close the password tomb using \u0026#39;pass close\u0026#39;. close your tomb 1 2 3 Œª ~/ pass close (*) Your password tomb has been closed. . Your passwords remain present in $HOME/.password.tomb. Let\u0026rsquo;s check what is happening with open and close options.\nWhen our tomb is closed, pass command and our passwords path looks like this:\n1 2 3 4 5 6 7 8 9 Œª ~/ pass Password Store ‚îî‚îÄ‚îÄ 2 Œª ~/ tree .password-store/ .password-store/ ‚îî‚îÄ‚îÄ 2 1 directory, 0 files We can not see our key:value (path/password-name) elements on output and filesystem.\nWhen opening our tomb we are decrypting our password database :\n1 2 3 4 5 6 7 8 9 10 11 Œª ~/ pass ; tree .password-store/ Password Store ‚îú‚îÄ‚îÄ lebaron.sh ‚îÇ¬†‚îî‚îÄ‚îÄ blog ‚îÇ¬†‚îî‚îÄ‚îÄ some_secret.gpg Œª ~/ tree .password-store/ .password-store/ ‚îú‚îÄ‚îÄ lebaron.sh ‚îÇ¬†‚îî‚îÄ‚îÄ blog ‚îÇ¬†‚îî‚îÄ‚îÄ some_secret.gpg Secrets with direnv and pass We can now manage to use our secrets in our Python code thanks to direnv and pass.\nWe need to modify a bit our .envrc in our project directory by checking if the tomb is open or not. I\u0026rsquo;m doing it by checking if I can stat my $HOME/.password-store/.gpg-id which is possible when the tomb is open. If not it means tomb is close and need to be open.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # .envrc layout pyenv 3.10.5 # POETRY if [[ ! -f pyproject.toml ]]; then log_status \u0026#39;No pyproject.toml found. Will initialize poetry in no-interactive mode\u0026#39; poetry init -n -q poetry run pip install -U pip wheel setuptools fi poetry run echo \u0026gt;\u0026gt; /dev/null local VENV=$(dirname $(poetry run which python)) export VIRTUAL_ENV=$(echo \u0026#34;$VENV\u0026#34; | rev | cut -d\u0026#39;/\u0026#39; -f2- | rev) export POETRY_ACTIVE=1 PATH_add \u0026#34;$VENV\u0026#34; if [ ! -L .venv ]; then ln -ns $VIRTUAL_ENV .venv fi # CHEKING IF¬†TOMB IS OPEN if ! stat $HOME/.password-store/.gpg-id \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ; then # IF¬†NOT OPEN IT pass open fi # THEN¬†PUT¬†SECRET¬†IN ENV export MY_SECRET=$(pass show lebaron.sh/blog/some_secret) We can now reload our direnv.\n1 2 3 4 5 6 Œª ~/project/ direnv reload direnv: loading ~/project/.envrc (*) Your password tomb has been opened in /home/$USER/.password-store/. . You can now use pass as usual. . When finished, close the password tomb using \u0026#39;pass close\u0026#39;. direnv: export +MY_SECRET +VIRTUAL_ENV ~PATH Notice here +MY_SECRET in the output, telling us it is accessible through our environment.\nWe can try to reach our secret :\n1 2 Œª ~/project/ echo $MY_SECRET my_secret What is really important here is that we are not storing clear secrets in our code, project or repo. All our secrets will be reachable through our virtual environment and can be used in you Python code like this :\n1 2 3 4 \u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; secret=os.environ.get(\u0026#39;MY_SECRET\u0026#39;) \u0026gt;\u0026gt;\u0026gt; print(secret) my_secret Direnv advanced configurations In more complex projects you may need commands, external dependencies, third-party, or third-party configuration. Let\u0026rsquo;s see how we can manage them with direnv.\nCheck for commands dependencies In your .envrc file, you can ensure you can use and reach some commands needed in your project.\nYou can do as follow:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # .envrc layout pyenv 3.10.5 # POETRY if [[ ! -f pyproject.toml ]]; then log_status \u0026#39;No pyproject.toml found. Will initialize poetry in no-interactive mode\u0026#39; poetry init -n -q poetry run pip install -U pip wheel setuptools fi poetry run echo \u0026gt;\u0026gt; /dev/null local VENV=$(dirname $(poetry run which python)) export VIRTUAL_ENV=$(echo \u0026#34;$VENV\u0026#34; | rev | cut -d\u0026#39;/\u0026#39; -f2- | rev) export POETRY_ACTIVE=1 PATH_add \u0026#34;$VENV\u0026#34; if [ ! -L .venv ]; then ln -ns $VIRTUAL_ENV .venv fi # CHEKING IF TOMB IS OPEN if ! stat $HOME/.password-store/.gpg-id \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ; then # IF NOT OPEN IT pass open fi # THEN PUT SECRET IN ENV export MY_SECRET=$(pass show lebaron.sh/blog/some_secret) # CHECKING COMMANDS DEPENDENCIES DIRENV_CMD_DEPENDENCIES=\u0026#34;unzip tar mkdir curl chmod rm\u0026#34; for mandatory_cmd in ${DIRENV_CMD_DEPENDENCIES}; do if [ -z \u0026#34;$(which ${mandatory_cmd})\u0026#34; ]; then echo \u0026#34;===\u0026gt; Mandatory command not found: ${mandatory_cmd}\u0026#34; exit 1 fi done Third-party binaries We can arrange to use specific third-party binaries such as packer, terraform, vault etc‚Ä¶\nWe will tell .direnv to use PATH in order to place our third-party binaries in .direnv/bin path.\nLet\u0026rsquo;s grab our .envrc file again:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # .envrc layout pyenv 3.10.5 # POETRY if [[ ! -f pyproject.toml ]]; then log_status \u0026#39;No pyproject.toml found. Will initialize poetry in no-interactive mode\u0026#39; poetry init -n -q poetry run pip install -U pip wheel setuptools fi poetry run echo \u0026gt;\u0026gt; /dev/null local VENV=$(dirname $(poetry run which python)) export VIRTUAL_ENV=$(echo \u0026#34;$VENV\u0026#34; | rev | cut -d\u0026#39;/\u0026#39; -f2- | rev) export POETRY_ACTIVE=1 PATH_add \u0026#34;$VENV\u0026#34; if [ ! -L .venv ]; then ln -ns $VIRTUAL_ENV .venv fi # CHEKING IF TOMB IS OPEN if ! stat $HOME/.password-store/.gpg-id \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ; then # IF NOT OPEN IT pass open fi # THEN PUT SECRET IN ENV export MY_SECRET=$(pass show lebaron.sh/blog/some_secret) # CHECKING COMMANDS DEPENDENCIES DIRENV_CMD_DEPENDENCIES=\u0026#34;unzip tar mkdir curl chmod rm\u0026#34; for mandatory_cmd in ${DIRENV_CMD_DEPENDENCIES}; do if [ -z \u0026#34;$(which ${mandatory_cmd})\u0026#34; ]; then echo \u0026#34;===\u0026gt; Mandatory command not found: ${mandatory_cmd}\u0026#34; exit 1 fi done export DIRENV_TMP_DIR=\u0026#34;${PWD}/.direnv\u0026#34; export DIRENV_BIN_DIR=\u0026#34;${DIRENV_TMP_DIR}/bin\u0026#34; if [ ! -e \u0026#34;${DIRENV_BIN_DIR}\u0026#34; ]; then mkdir -p \u0026#34;${DIRENV_BIN_DIR}\u0026#34; fi export PATH=\u0026#34;${DIRENV_BIN_DIR}:${PATH}\u0026#34; Now install our binary, let\u0026rsquo;s try it with packer Hashicorp binary :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # .envrc layout pyenv 3.10.5 # POETRY if [[ ! -f pyproject.toml ]]; then log_status \u0026#39;No pyproject.toml found. Will initialize poetry in no-interactive mode\u0026#39; poetry init -n -q poetry run pip install -U pip wheel setuptools fi poetry run echo \u0026gt;\u0026gt; /dev/null local VENV=$(dirname $(poetry run which python)) export VIRTUAL_ENV=$(echo \u0026#34;$VENV\u0026#34; | rev | cut -d\u0026#39;/\u0026#39; -f2- | rev) export POETRY_ACTIVE=1 PATH_add \u0026#34;$VENV\u0026#34; if [ ! -L .venv ]; then ln -ns $VIRTUAL_ENV .venv fi # CHEKING IF TOMB IS OPEN if ! stat $HOME/.password-store/.gpg-id \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ; then # IF NOT OPEN IT pass open fi # THEN PUT SECRET IN ENV export MY_SECRET=$(pass show lebaron.sh/blog/some_secret) # CHECKING COMMANDS DEPENDENCIES DIRENV_CMD_DEPENDENCIES=\u0026#34;unzip tar mkdir curl chmod rm\u0026#34; for mandatory_cmd in ${DIRENV_CMD_DEPENDENCIES}; do if [ -z \u0026#34;$(which ${mandatory_cmd})\u0026#34; ]; then echo \u0026#34;===\u0026gt; Mandatory command not found: ${mandatory_cmd}\u0026#34; exit 1 fi done export DIRENV_TMP_DIR=\u0026#34;${PWD}/.direnv\u0026#34; export DIRENV_BIN_DIR=\u0026#34;${DIRENV_TMP_DIR}/bin\u0026#34; if [ ! -e \u0026#34;${DIRENV_BIN_DIR}\u0026#34; ]; then mkdir -p \u0026#34;${DIRENV_BIN_DIR}\u0026#34; fi export PATH=\u0026#34;${DIRENV_BIN_DIR}:${PATH}\u0026#34; # PACKER INSTALLATION PACKER_VERSION=\u0026#34;1.8.1\u0026#34; PACKER_ARCH=\u0026#34;linux_amd64\u0026#34; PACKER_PKG_NAME=\u0026#34;packer_${PACKER_VERSION}_${PACKER_ARCH}.zip\u0026#34; PACKER_PKG_URL=\u0026#34;https://releases.hashicorp.com/packer/${PACKER_VERSION}/${PACKER_PKG_NAME}\u0026#34; PACKER_PKG_PATH=\u0026#34;${DIRENV_TMP_DIR}/${PACKER_PKG_NAME}\u0026#34; if [ ! -e \u0026#34;${DIRENV_BIN_DIR}/packer\u0026#34; ]; then echo \u0026#34;===\u0026gt; Getting packer:${PACKER_VERSION}:${PACKER_ARCH} (can take a while to execute)\u0026#34; curl -s -L \u0026#34;${PACKER_PKG_URL}\u0026#34; -o \u0026#34;${PACKER_PKG_PATH}\u0026#34; unzip ${PACKER_PKG_PATH} -d ${DIRENV_BIN_DIR} chmod 700 ${DIRENV_BIN_DIR}/packer rm -f ${PACKER_PKG_PATH} fi Let\u0026rsquo;s try out our new configuration :\n1 2 3 4 5 6 Œª ~/project direnv allow direnv: loading ~/project/.envrc ===\u0026gt; Getting packer:1.8.1:linux_amd64 (can take a while to execute) Archive: ~/project/.direnv/packer_1.8.1_linux_amd64.zip inflating: ~/project/.direnv/bin/packer direnv: export +MY_SECRET +VIRTUAL_ENV ~PATH Now packer is installed in our PATH let\u0026rsquo;s locate it and run it :\n1 2 3 4 Œª ~/project/ which packer ~/project/.direnv/bin/packer Œª ~/ packer --version 1.8.1 If in the futur you need to change packer version you just have to remove your current packer binary and modify PACKER_VERSION variable to rebuild your direnv.\nThird-party configs Of course, if our third-party need custom (or not) configurations we can specify them in our .envrc but to keep pour code clear and organized we also can split our configs into subfiles.\nBy adding this block at the end of our .envrc file :\n1 2 3 4 5 6 7 8 9 10 11 # .envrc # [‚Ä¶] # ADDONS ENV_ADDONS=\u0026#34;.env.packer .env.custom_config\u0026#34; for addon in ${ENV_ADDONS}; do if [ -e \u0026#34;${PWD}/${addon}\u0026#34; ]; then source ${PWD}/${addon} fi done And then create your addons custom config file :\n1 2 3 # .env.packer export PACKER_VAR1=VAR1_VALUE export PACKER_VAR2=VAR2_VALUE 1 2 3 # .env.custom_config export CUSTOM_VAR1=CUSTOM1_VALUE export CUSTOM_VAR2=CUSTOM2_VALUE Don\u0026rsquo;t forget to reload:\n1 Œª ~/project/ direnv reload Final template Here is a final .envrc template you can grab and edit for your needs :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 # .envrc layout pyenv 3.10.5 # POETRY if [[ ! -f pyproject.toml ]]; then log_status \u0026#39;No pyproject.toml found. Will initialize poetry in no-interactive mode\u0026#39; poetry init -n -q poetry run pip install -U pip wheel setuptools fi poetry run echo \u0026gt;\u0026gt; /dev/null local VENV=$(dirname $(poetry run which python)) export VIRTUAL_ENV=$(echo \u0026#34;$VENV\u0026#34; | rev | cut -d\u0026#39;/\u0026#39; -f2- | rev) export POETRY_ACTIVE=1 PATH_add \u0026#34;$VENV\u0026#34; if [ ! -L .venv ]; then ln -ns $VIRTUAL_ENV .venv fi # CHEKING IF TOMB IS OPEN if ! stat $HOME/.password-store/.gpg-id \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ; then # IF NOT OPEN IT pass open fi # THEN PUT SECRET IN ENV export MY_SECRET=$(pass show lebaron.sh/blog/some_secret) # CHECKING COMMANDS DEPENDENCIES DIRENV_CMD_DEPENDENCIES=\u0026#34;unzip tar mkdir curl chmod rm\u0026#34; for mandatory_cmd in ${DIRENV_CMD_DEPENDENCIES}; do if [ -z \u0026#34;$(which ${mandatory_cmd})\u0026#34; ]; then echo \u0026#34;===\u0026gt; Mandatory command not found: ${mandatory_cmd}\u0026#34; exit 1 fi done export DIRENV_TMP_DIR=\u0026#34;${PWD}/.direnv\u0026#34; export DIRENV_BIN_DIR=\u0026#34;${DIRENV_TMP_DIR}/bin\u0026#34; if [ ! -e \u0026#34;${DIRENV_BIN_DIR}\u0026#34; ]; then mkdir -p \u0026#34;${DIRENV_BIN_DIR}\u0026#34; fi export PATH=\u0026#34;${DIRENV_BIN_DIR}:${PATH}\u0026#34; # PACKER INSTALLATION PACKER_VERSION=\u0026#34;1.8.1\u0026#34; PACKER_ARCH=\u0026#34;linux_amd64\u0026#34; PACKER_PKG_NAME=\u0026#34;packer_${PACKER_VERSION}_${PACKER_ARCH}.zip\u0026#34; PACKER_PKG_URL=\u0026#34;https://releases.hashicorp.com/packer/${PACKER_VERSION}/${PACKER_PKG_NAME}\u0026#34; PACKER_PKG_PATH=\u0026#34;${DIRENV_TMP_DIR}/${PACKER_PKG_NAME}\u0026#34; if [ ! -e \u0026#34;${DIRENV_BIN_DIR}/packer\u0026#34; ]; then echo \u0026#34;===\u0026gt; Getting packer:${PACKER_VERSION}:${PACKER_ARCH} (can take a while to execute)\u0026#34; curl -s -L \u0026#34;${PACKER_PKG_URL}\u0026#34; -o \u0026#34;${PACKER_PKG_PATH}\u0026#34; unzip ${PACKER_PKG_PATH} -d ${DIRENV_BIN_DIR} chmod 700 ${DIRENV_BIN_DIR}/packer rm -f ${PACKER_PKG_PATH} fi # ADDONS ENV_ADDONS=\u0026#34;.env.packer .env.custom_config\u0026#34; for addon in ${ENV_ADDONS}; do if [ -e \u0026#34;${PWD}/${addon}\u0026#34; ]; then source ${PWD}/${addon} fi done Conclusion Yes this require a bit of knowledge and a some configurations to be efficient in this workflow, but keep in mind that all will be configured automatically and will be more easier version controlled.\nYou can arrange in order to put your application in production area to share your GPG key or to manage multiple GPG key in a same pass (eg one GPG¬†ID for each team member), version your pass and make it a git version accessible.\nAll dependencies, with the same packages will be deployed exactly as your developpment area.\nKeep it simple.\nStarter template is available here.\n","date":"2022-06-10T08:12:12+02:00","permalink":"https://lebaron.sh/p/ultimate-python-development-environment-configuration/","title":"Ultimate Python development environment configuration"},{"content":"When I was working at OVHCloud company, I\u0026rsquo;ve developed a feature called BYOI (Bring Your Own Image).\nBring Your Own Image technology allows you to boot any cloud (or not) images on a baremetal.\nEven if today not all editors are ready to provide completely agnostic images (and I mean UEFI ready and/or legacy boot), by triturating (packer) a bit the whole, we can have something functional.\nThis mark the first step towards the Hybrid IAC.\nThe documentation is here\nWhat does it really do?\nAPI deployment (+automation) post-install scripting (+automation) transparent installation and system (+security) template customization (+security) nova boot custom (+tech) A little warning, as I said above, not all images are ready to boot automatically yet, there are many bug-tracks opened on different editors because of unwanted behaviors at boot time, grub config, init phase etc\nIf you see these errors via a KVM/IPMI, it means that the installation went well, but the sticking point is not the deployment technology itself, but the image installed on the disks.\n","date":"2020-07-20T12:00:00Z","permalink":"https://lebaron.sh/p/bring-your-own-image/","title":"Bring Your Own Image"},{"content":"I\u0026rsquo;ve been looking for a way to break my routine a bit when I\u0026rsquo;m working on my laptop. I figured that changing the wallpaper randomly and automatically was a good way to break the monotony.\nI use awesomeWM (version 4)\n1 2 3 4 5 6 7 [f00b@void ~]$ awesome --version awesome v4.3 (Too long) ‚Ä¢ Compiled against Lua 5.3.5 (running with Lua 5.3) ‚Ä¢ D-Bus support: ‚úî ‚Ä¢ execinfo support: ‚úò ‚Ä¢ xcb-randr version: 1.6 ‚Ä¢ LGI version: 0.9.2 So I needed three things:\nA folder full of images A little script that will choose one at random A call to this script from awesome init For the images, I use the excellent repo by Luck Smith.\nAs far as the script is concerned, nothing too hard:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #!/bin/bash # author : lebarondecharlus # descr : Make your wallpaper change on each start ! # # I\u0026#39;m using Luck Smith wallpaper git repo for all images # link : https://github.com/LukeSmithxyz/wallpapers # current awesome theme THEME=\u0026#34;powerarrow-dark\u0026#34; # Awesome conf path AWPATH=\u0026#34;$HOME/.config/awesome/themes/$THEME\u0026#34; # image should have absolute path to image folder IMAGE=$(find $HOME/Pictures/wallpapers/ -type f -name \u0026#34;*.png\u0026#34; -o -name \u0026#34;*.jpeg\u0026#34; -o -name \u0026#34;*.jpg\u0026#34;| shuf -n 1 | sed \u0026#39;s/\\ /\\\\ /g\u0026#39;) cp -f $IMAGE $AWPATH/wall.png # don\u0026#39;t forget to add those lines at the end of your rc.lua (replace with your correct path and script name) # # -- Startup programs # awful.util.spawn_with_shell(\u0026#34;~/bin/wallpaper.sh\u0026#34;) As indicated in comment, just add these two lines (or just one without the comment) to call the script via awesomeWM init.\nPut this at the end of ~/.config/awesome/rc.lua.\n1 2 -- Startup programs awful.util.spawn_with_shell(\u0026#34;~/bin/wallpaper.sh\u0026#34;) On each awesomewm restart, you will have a new pretty (or not) wallpaper.\nScript is on gist !\n","date":"2020-01-10T11:10:33Z","permalink":"https://lebaron.sh/p/random-wallpaper-with-awesomewm/","title":"Random wallpaper with AwesomeWM¬†!"},{"content":"When you create an OVH Public Cloud instance under Freebsd with a certain amount of disk space, let‚Äôs say 50G, you will find that it is not applied on your partition.\nFirst let\u0026rsquo;s look at what we have:\n1 2 3 4 5 6 # gpart show =\u0026gt; 40 10239920 da0 GPT (50G) [CORRUPT] 40 1024 1 freebsd-boot (512K) 1064 984 - free - (492K) 2048 10235904 2 freebsd-zfs (4.9G) 10237952 2008 - free - (1.0M) We note that our volume da0 is tagged as CORRUPT. Don\u0026rsquo;t panic, everyone knows that the Freebsd handbook is great. I quote:\nIf the disk was formatted with the GPT partitioning scheme, it may show as ‚Äúcorrupted‚Äù because the GPT backup partition table is no longer at the end of the drive. Fix the backup partition table with gpart:\n1 2 # gpart recover ada0 ada0 recovered Well, let\u0026rsquo;s apply this to our server by replacing ada0 by da0 :\n1 2 # gpart recover da0 da0 recovered Check :\n1 2 3 4 5 6 # gpart show =\u0026gt; 40 104857520 da0 GPT (50G) 40 1024 1 freebsd-boot (512K) 1064 984 - free - (492K) 2048 10235904 2 freebsd-zfs (4.9G) 10237952 94619608 - free - (45G) Much better! We see that our da0 \u0026ldquo;disk\u0026rdquo; has 50G. However if we look more closely at our system, we see that not all the space is present.\n1 2 3 4 5 # df -h Filesystem Size Used Avail Capacity Mounted on zroot/ROOT/default 4.7G 493M 4.2G 10% / devfs 1.0K 1.0K 0B 100% /dev zroot 4.2G 96K 4.2G 0% /zroot Once again, don\u0026rsquo;t panic. The handbook is our friend.\nLet\u0026rsquo;s apply the 45G free on our score :\n1 2 # gpart resize -i 2 -a 4k -s 50G da0 da0p2 resized Check :\n1 2 3 4 5 6 # gpart show =\u0026gt; 40 104857520 da0 GPT (50G) 40 1024 1 freebsd-boot (512K) 1064 984 - free - (492K) 2048 94371840 2 freebsd-zfs (45G) 94373888 10483672 - free - (5.0G) Well, we are moving forward, however, the space is not yet usable as the return from df :\n1 2 3 4 5 # df -h Filesystem Size Used Avail Capacity Mounted on zroot/ROOT/default 4.7G 493M 4.2G 10% / devfs 1.0K 1.0K 0B 100% /dev zroot 4.2G 96K 4.2G 0% /zroot We must ask to our zpool to use this space.\nLet‚Äôs first check our pool.\n1 2 3 4 5 6 7 8 9 # zpool status pool: zroot state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM zroot ONLINE 0 0 0 da0p2 ONLINE 0 0 0 Ask it we want to autoexpand on zroot\n1 # zpool set autoexpand=on zroot Apply it on da0p2 :\n1 # zpool online -e zroot /dev/da0p2 Last check :\n1 2 3 4 5 # df -h Filesystem Size Used Avail Capacity Mounted on zroot/ROOT/default 44G 493M 43G 1% / devfs 1.0K 1.0K 0B 100% /dev zroot 43G 96K 43G 0% /zroot This is it !\n","date":"2018-03-04T14:56:33Z","permalink":"https://lebaron.sh/p/freebsd-pci-disk-space/","title":"Freebsd Pci Disk Space"},{"content":"You may have noticed it, but when you populate a PCI OVH instance under Ubuntu by activating Vrack, your Vm does not have its private IP at boot time. So, yes, I don\u0026rsquo;t like Ubuntu, but sometimes you don\u0026rsquo;t have a choice.\nAnyway, all this to say that we don\u0026rsquo;t have our private IP and it\u0026rsquo;s too sad. (RT)\nThe fix trick is stupid. Very stupid.\nAdd:\n1 2 allow-hotplug ens4 iface ens4 inet dhcp In /etc/network/interface file.\nThen :\n1 systemctl restart network and\n1 ifup ens4 That‚Äôs it\nI‚Äôve sent an email on OVH\u0026rsquo;s ML [cloud], because I still found it strange that this bug still exists.\nIt was on 21 November 2017.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Hello la team, Juste une petite remarque sur l\u0026#39;installation d\u0026#39;un Ubuntu PCI avec Vrack. En fait je suis oblig√© d\u0026#39;ajouter : allow-hotplug ens4 iface ens4 inet dhcp Dans /etc/network/interface puis systemctl restart network et ifup ens4 Il me semble que sur les autres distrib\u0026#39; les confs network s\u0026#39;ajoutent automatiquement √† l\u0026#39;install non ? Je passe peut-√™tre √† c√¥t√© d\u0026#39;un truc... en m√™me temps je ne connais pas bien les syst√®mes en .deb... Bises √† tout le monde F00b4rch Answer :\n1 2 3 4 5 6 21/11/2017 √Ä cloud Hello, Exact, sur debian 9 (voir 8), les interfaces sont mont√©e automatiquement... Je crois qu\u0026#39;il a un bug d\u0026#39;ouvert cote ubuntu pour √ßa, si je le retrouve je te l\u0026#39;envoi. ","date":"2018-03-04T14:41:25Z","permalink":"https://lebaron.sh/p/ubuntu-vrack-ovh-fix/","title":"Ubuntu Vrack Ovh Fix"},{"content":"Now that Gitlab offers its own image registry, it is possible to use it directly in our K8s environment! If you missed the info (which is starting to date now), I refer you to this article.\nTo add the Gitlab private registry in Kubernetes you have to create a secret :\n\u0026gt; kubectl create secret docker-registry regsecret --docker-server=registry.gitlab.xyz --docker-username='' --docker-password='' --docker-email=\u0026quot;\u0026quot; Where :\n1 2 3 4 --docker-server regitry gitlab --docker-username user gitlab autoris√© √† acceder au registry --docker-password son mot de passe --docker-email son email We will check secret creation :\n1 2 3 \u0026gt; kubectl get secret regsecret NAME TYPE DATA AGE regsecret kubernetes.io/dockercfg 1 19h Show details :\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; kubectl get secret regsecret --output=yaml apiVersion: v1 data: .dockercfg: eyJiXXXXXXteoirutXXXXetusrnXX= kind: Secret metadata: creationTimestamp: 2017-08-07T13:32:09Z name: regsecret namespace: default resourceVersion: \u0026#34;2783\u0026#34; selfLink: /api/v1/namespaces/default/secrets/regsecret uid: cfeXXXb7-7b74-XXX-XXX907-4iu8237492 type: kubernetes.io/dockercfg It‚Äôs now possible to use images from your registry directly in your deployment.\nExample\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: apps/v1beta1 kind: Deployment metadata: name: test-registry spec: replicas: 3 template: metadata: labels: app: my-app version: v1 spec: containers: - name: my-app image: registry.gitlab.xyz/images/my-app:latest imagePullPolicy: Always ports: - containerPort: 80 imagePullSecrets: - name: regsecret See where we call secret :\n1 2 imagePullSecrets: - name: regsecret Enjoy !\n","date":"2017-08-10T12:06:05Z","permalink":"https://lebaron.sh/p/add-gitlab-registry-in-kubernetes/","title":"Add Gitlab Registry in Kubernetes"},{"content":"With OVH Public Cloud, it is possible for you to control your OpenStack instances directly from the Nova client.\nIt is rather practical to go through the command line rather than having to access the manager which is often, let us say it, slow.\nBut to avoid having to prepare each time a working environment compatible with Nova, I find more interesting to directly create a Docker image for this purpose. In this way, no more need to install anything except the Docker daemon on its workstation.\nThe DockerFile looks like :\n1 2 3 4 5 6 7 8 9 10 11 12 13 FROM debian:latest RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y \\ python-glanceclient \\ python-novaclient env OS_AUTH_URL=\u0026#34;\u0026#34; env OS_TENANT_ID=\u0026#34;\u0026#34; env OS_TENANT_NAME=\u0026#34;\u0026#34; env OS_USERNAME=\u0026#34;\u0026#34; env OS_PASSWORD=\u0026#34;\u0026#34; env OS_REGION_NAME=\u0026#34;\u0026#34; Note that I use the basic debian image because I already have it locally, but you can replace the bone with the one you want and adapt the content of RUN.\nYou also need to fill the environment variables with information about your OpenStack env.\nLet‚Äôs go with :\nsudo docker build -t clientNova . Then run your container as follow :\nsudo docker run --rm -it novaClient bash You can add alias to your .bashrc / .zshrc\nalias nova=\u0026quot;sudo docker run --rm -it novaClient bash\u0026quot; Sources are available !\nEnjoy !\n","date":"2017-08-08T13:52:07Z","permalink":"https://lebaron.sh/p/dockerize-nova-client/","title":"Dockerize Nova client"},{"content":"\nThe world of hosting is changing, and so is the world of application development. Today we are turning less and less to dedicated hosting for a single application, but more to build the infrastructure that will support it.\nIn this sense, we prefer to use an Iaas solution on dedicated \u0026ldquo;bare metal\u0026rdquo; for our application overlay than pure \u0026ldquo;bare metal\u0026rdquo; per service.\nApplication deployment missions pushed by developers must fit with the technology and logic of production. Which pipelines should we use for our CI, CD ?\nThis post will not aim at answering the question of the pipelines to be implemented, it will be a question of Ranching, coupled with its Cattle orchestrator, on Public Cloud OVH Openstack.\nInstallation : The installation of PCI (Public Cloud) instances is the fastest step. We need to start with 5 instances:\n1 LoadBalancer/ReverseProxy (HA-Proxy or Nginx) for ‚Ç¨299 3 RancherServer in Cluster under Galera at 5‚Ç¨99 1 Node Worker (for our applications) at the price you want to put for your performance #### LoadBalancer Nginx : For Nginx installation, nothing very complex, a ‚Ç¨2.99 instance will be more than enough since the only job of the server will be to forward the request to our rancher cluster.\nThe conf file will be :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 upstream rancher { server rancher-server1:8080; server rancher-server2:8080; server rancher-server3:8080; } map $http_upgrade $connection_upgrade { default Upgrade; \u0026#39;\u0026#39; close; } server { listen 443 ssl spdy; server_name \u0026lt;server\u0026gt;; ssl_certificate \u0026lt;cert_file\u0026gt;; ssl_certificate_key \u0026lt;key_file\u0026gt;; location / { proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Forwarded-Port $server_port; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://rancher; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; proxy_read_timeout 900s; } } server { listen 80; server_name \u0026lt;server\u0026gt;; return 301 https://$server_name$request_uri; } Replace the upstream by the ips of your servers.\nNote that it is also possible to dockerize this service. This way, the day you put a second LB on the front end, there will only be one container to place on your instance.\nRancher installation : For Rancher part, there are no great difficulties either. 5‚Ç¨99 instances will be more than enough for the needs.\nI recommend the official Rancher documentation on this subject.\nWe will start by deploying on each Rancher server:\n1 sudo docker run -d --restart=unless-stopped -p 8080:8080 rancher/server Then we will add the servers to each other via the UI. (Infrastructure \u0026gt; Hosts \u0026gt; Add Host)\nYou should get the following code to run on each node :\n1 sudo docker run -e CATTLE_AGENT_IP=\u0026#34;1.2.3.4\u0026#34; -e CATTLE_HOST_LABELS=\u0026#39;galera=true\u0026#39; --rm --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/rancher:/var/lib/rancher rancher/agent:v1.2.5 http://1.2.3.4:8080/v1/scripts/4956918455D4D9BE3AF1:1483142400000:Fscj9CvRSrx0mS05E4kdWDkb0E Once this step is done, it will then be possible to use the Galera image proposed by Rancher (in the catalog) on our 3 servers. A node will then be present on each server.\nWe will be able to initialize the cluster on a Galera node:\n1 2 3 \u0026gt; CREATE DATABASE IF NOT EXISTS cattle COLLATE = \u0026#39;utf8_general_ci\u0026#39; CHARACTER SET = \u0026#39;utf8\u0026#39;; \u0026gt; GRANT ALL ON cattle.* TO \u0026#39;cattle\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;cattle\u0026#39;; \u0026gt; GRANT ALL ON cattle.* TO \u0026#39;cattle\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;cattle\u0026#39;; Then check on the other two servers that the basic entries are also present.\nThen let\u0026rsquo;s stop one Rancher server and start it using Galera for persistence of its data :\n1 2 sudo docker run -d --restart=unless-stopped -p 8080:8080 rancher/server \\ --db-host myhost.example.com --db-port 3306 --db-user username --db-pass password --db-name cattle Where :\n1 2 3 4 5 --db-host IP du serveur MySQL --db-port port du serveur MySQL (default: 3306) --db-user username MySQL login (default: cattle) --db-pass password MySQL login (default: cattle) --db-name nom de la base MySQL (default: cattle) Do the same for the other two servers so each Rancher launches using the Galera database.\nExpected result: It is possible to check if Rancher is clustered via the UI in: Admin \u0026gt; High Availability.\nAddition of Worker Nodes. You now have your Rancher cluster with a front-end LB, it is now possible to add Worker nodes to your cluster. It\u0026rsquo;s very easy to add them directly with the Rancher utility. Now it\u0026rsquo;s up to you to see which type of instance best suits your resource needs.\nRancher\u0026rsquo;s default environment uses the Cattle' orchestrator, so once your Workers nodes are configured, you can deploy your Docker\u0026rsquo; containers directly from your cluster.\nConclusion The installation of a Rancher environment is fast, the Public Cloud OVH allows to quickly deploy the necessary instances of Rancher. The ease of use offered by the Rancher/Cattle duo allows an efficient and fluid commissioning.\nWe will see in a next article how to set up an HA environment with Kubernetes, still under the OVH PCI and using the environment template proposed by Rancher.\n","date":"2017-08-04T19:04:11Z","permalink":"https://lebaron.sh/p/setup-rancher-cluster-on-ovh-public-cloud/","title":"Setup Rancher cluster on OVH Public Cloud"},{"content":"We need servers at Online, but there is no availability! So they came to ask me if I didn\u0026rsquo;t have a magic solution\u0026hellip;\nA little bash\u0026hellip; a notify in this case Slack and here we go!\nAs usual sources are available.\nDirty way *To be alerted via slack you have to create an incoming-webhook which will generate a link.\nFor an XC 2016 series server:\ntext=\u0026quot;DISPO : https://www.online.net/fr/serveur-dedie/dedibox-xc\u0026quot;; json=\u0026quot;{\\\u0026quot;channel\\\u0026quot;: \\\u0026quot;#infra\\\u0026quot;, \\\u0026quot;text\\\u0026quot;: \\\u0026quot;$text\\\u0026quot;}\u0026quot; ; while true ; do curl --silent https://www.online.net/fr/serveur-dedie | grep '\u0026lt;button class=\u0026quot;btn btn--primary js-order-dedibox\u0026quot;' | grep -i 'xc 2016' | grep -i 'victime' || curl -s -d \u0026quot;payload=$json\u0026quot; \u0026quot;https://hooks.slack.com/services/XXX/XXXX/XXXX/XXXX\u0026quot; ; sleep 5 ; done Curl will be done only if one server is available.\nClean way Just code formating :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash webhook=\u0026#34;https://hooks.slack.com/services/XXX/XXXX/XXXX/XXXX\u0026#34; url=\u0026#34;https://www.online.net/fr/serveur-dedie/XXXXX\u0026#34; text=\u0026#34;DISPO : $url\u0026#34; channel=\u0026#34;test\u0026#34; json=\u0026#34;{\\\u0026#34;channel\\\u0026#34;: \\\u0026#34;#$channel\\\u0026#34;, \\\u0026#34;text\\\u0026#34;: \\\u0026#34;$text\\\u0026#34;}\u0026#34; server=\u0026#34;XC 2016\u0026#34; while true do curl --silent https://www.online.net/fr/serveur-dedie | \\ grep \u0026#39;\u0026lt;button class=\u0026#34;btn btn--primary js-order-dedibox\u0026#34;\u0026#39; | \\ grep -i \u0026#34;$server\u0026#34; | grep -i \u0026#39;victime\u0026#39; || \\ curl -s -d \u0026#34;payload=$json\u0026#34; \u0026#34;$webhook\u0026#34; ; \\ sleep 5 ; \\ done You can also push the vice further by filtering on the region and the disc types.\nPig mode Because\u0026hellip; why not ?\n1 text=\u0026#34;DISPO https://www.online.net/fr/serveur-dedie/dedibox-xc\u0026#34;; json=\u0026#34;{\\\u0026#34;channel\\\u0026#34;: \\\u0026#34;#test\\\u0026#34;, \\\u0026#34;text\\\u0026#34;: \\\u0026#34;$text\\\u0026#34;}\u0026#34; ; while true ; do curl --silent https://www.online.net/fr/serveur-dedie/dedibox-xc | egrep -io \u0026#39;\u0026lt;option value=\u0026#34;\\w+\u0026#34;\u0026gt;ssd / france / dc2\u0026lt;/option\u0026gt;\u0026#39; \u0026amp;\u0026amp; curl -s -d \u0026#34;payload=$json\u0026#34; \u0026#34;https://hooks.slack.com/services/XXX/XXXX/XXXX\u0026#34; ; sleep 5 ; done Next step, simulate an order ?\n","date":"2017-08-01T09:40:07Z","permalink":"https://lebaron.sh/p/online-servers-availability/","title":"Online servers availability"},{"content":"I recently had to migrate containers from a proxmox3 (under OpenVZ) to a proxmox4 (under LXC).\nProblem, there are a lot of containers to migrate/\u0026ldquo;convert\u0026rdquo; to run under LXC. So I needed a way to automate the procedure as much as possible.\nLuckily, the migration documentation is very well detailed. So I used it to \u0026ldquo;bash\u0026rdquo; the operation.\nYou can find all the sources on my github\nI cut the operation under two scripts, an export script and an import script.\nThe export : The export script takes two parameters as input: the container ID and the IP of the destination server.\nIt is also necessary to define some variables that will be used to send data via scp.\nThe export procedure is done as follows:\n1 2 3 4 5 # Stop \u0026amp; Dump sudo vzctl stop $ID \u0026amp;\u0026amp; \\ echo \u0026#34;$ID stopped [OK]\u0026#34; \u0026amp;\u0026amp; \\ sudo vzdump $ID -dumpdir /home/$USER/vzdump \u0026amp;\u0026amp; \\ echo \u0026#34;$ID : dump [OK]\u0026#34; \u0026amp;\u0026amp; \\ We check dump is present :\n1 2 3 4 5 6 7 # DumpName vzDumpName=$(ls /home/$USER/vzdump/) if [ -z $vzDumpName ] ; then echo \u0026#34;No dump found in /home/$USER/vzdump/\u0026#34; exit fi Then we send it to Proxmox 4 :\n1 2 3 4 5 6 7 sudo scp -i /home/$USER/.ssh/id_rsa \u0026#34;-P $rPort\u0026#34; $vzDumpName $rUSER@$rIP:$rPath \u0026amp;\u0026amp; \\ echo \u0026#34;Sending Dump [OK]\u0026#34; \u0026amp;\u0026amp; \\ sudo scp -i /home/$USER/.ssh/id_rsa \u0026#34;-P $rPort\u0026#34; vz.log $rUSER@$rIP:$rPath \u0026amp;\u0026amp; \\ echo \u0026#34;Sending Log [OK]\u0026#34; \u0026amp;\u0026amp; \\ sudo rm $vzDumpName \u0026amp;\u0026amp; \\ sudo rm vz.log \u0026amp;\u0026amp; \\ echo \u0026#34;SCP $vzDumpName on $rIP [OK]\u0026#34; Import : As for the import, only one argument is passed as input, which is the container\u0026rsquo;s \u0026ldquo;ID\u0026rdquo;.\nThen we start the conversion and restoration part.\n1 2 3 4 5 6 7 8 9 sudo pct restore $ID $dumpPath/$dumpName \u0026amp;\u0026amp; \\ echo \u0026#34;pct $ID restoring [OK]\u0026#34; # At this poin, you can set network configuration # exemple : pct set 101 -net0 name=eth0,bridge=vmbr0,ip=192.168.15.144/24,gw=192.168.15.1 # I prefer doing it manually sudo pct start $ID sudo pct enter $ID I omitted to tell you that there is a small logging system for the operations in order to be able to trace the process a bit.\n","date":"2017-07-31T18:15:50Z","permalink":"https://lebaron.sh/p/migrate-openvz-to-lxc/","title":"Migrate Openvz To Lxc"},{"content":"123 To add an additional disk/volume on your OVH public cloud, you need to follow some steps.\nFirst, identify your new disk :\nfdisk -l You can have different output depending of your system (sd{x}, vd{x}).\nThen create a new partition :\n1 2 3 4 # parted /dev/{{disk}} mktable gpt mkpart primary ext4 512 100% quit Format it :\nmkfs -t ext4 -L rootfs /dev/{{disk}}1 Mount :\nmount /dev/{{disk}}1 /mnt Let‚Äôs make it peristent, we need UUID. To get block ID.\n1 2 3 4 # blkid /dev/sdb1: LABEL=\u0026#34;rootfs\u0026#34; UUID=\u0026#34;6b75bbb4-b311-4b9d-a8fd-6e6ff23c401f\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTLABEL=\u0026#34;primary\u0026#34; PARTUUID=\u0026#34;e20dc227-9d10-41c4-a714-2fb53d190c11\u0026#34; /dev/sda1: UUID=\u0026#34;9abb590f-8a5e-496f-ad2a-2c877415bdc5\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;71036eb1-01\u0026#34; Add it on /etc/fstab file with mount information.\n1 2 3 # vim /etc/fstab [...] UUID=6b75bbb4-b311-4b9d-a8fd-6e6ff23c401f /mnt ext4 errors=remount-ro,discard 0 1 Confirm good configuration with one reboot.\n","date":"2017-07-31T11:38:18Z","permalink":"https://lebaron.sh/p/additional-volume-public-cloud-ovh/","title":"Additional Volume Public Cloud Ovh"},{"content":"Monitoring a database in standalone mode is one thing, but when it comes to clustering, it\u0026rsquo;s a little more complex.\nThis is the case with Galera clustering (mariaDB/mysql). Zabbix (\u0026amp;co) offered me simple solutions for single database servers, but I didn\u0026rsquo;t find a really interesting template for monitoring a Galera cluster for production.\nSo several questions, what to monitor, how to alert, what\u0026rsquo;s the best method?\nI based myself on the official Galera documentation to have all the important elements to monitor. For the choice of the Golang language, it seemed to me that it provided me with the necessary functionalities. As for the choice of alerting, I decided to use a Slack App.\nYou can find the sources of my project here.\nExample of a healthy output :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026gt; go run main.go ### Version #### 2017/06/16 14:19:48 Serveur n1 - version 5.6.35-1xenial 2017/06/16 14:19:48 Serveur n2 - version 5.6.35-1xenial 2017/06/16 14:19:48 Serveur n3 - version 5.6.35-1xenial ### UUID ### 2017/06/16 14:19:48 n1 a1c404a9-51b4-11e7-b057-237cc5970d38 2017/06/16 14:19:48 n2 a1c404a9-51b4-11e7-b057-237cc5970d38 2017/06/16 14:19:48 n3 a1c404a9-51b4-11e7-b057-237cc5970d38 ### Nodes ### 2017/06/16 14:19:48 Total Nodes : 3 2017/06/16 14:19:48 Number of Nodes counts : 3 2017/06/16 14:19:48 Number of Nodes counts : 3 2017/06/16 14:19:48 Number of Nodes counts : 3 ### STATUS ### 2017/06/16 14:19:48 n1 status : Primary 2017/06/16 14:19:48 n2 status : Primary 2017/06/16 14:19:48 n3 status : Primary 2017/06/16 14:19:48 n1 is ready : [ON] 2017/06/16 14:19:48 n2 is ready : [ON] 2017/06/16 14:19:48 n3 is ready : [ON] 2017/06/16 14:19:48 n1 is connected : [ON] 2017/06/16 14:19:48 n2 is connected : [ON] 2017/06/16 14:19:48 n3 is connected : [ON] ### Average Replication ### 2017/06/16 14:19:48 Average on n2 : 0.000000 2017/06/16 14:19:48 Average on n3 : 0.000000 2017/06/16 14:19:48 Average on n1 : 0.100000 You have to use the slackApp package in which I placed an exposed function PayloadSlack() to then customize your alerts.\nThe whole output is not displayed in the STDOUT when it is healthy. Nevertheless you just have to uncomment the println in the main.go to be able to perform your tests.\n","date":"2017-07-30T15:08:41Z","permalink":"https://lebaron.sh/p/galera-monitoring/","title":"Galera Monitoring"},{"content":"To automatically update the list of templates available via prox in your local space just use the following command:\npveam update I found it good to put it in cron once a week so as not to have too much space for minor versions.\n","date":"2017-07-30T14:53:16Z","permalink":"https://lebaron.sh/p/update-proxmox-templates/","title":"Update Proxmox templates"},{"content":"On one specific request, I had to work on the elaboration of an automating program reacting on SFTP users updates.\nThe main technical issue of this request is that the SFTP protocol does not have a logging system.\nI had heard about the pyinotify library so I started working on it.\nThe project is presented in its primary mechanism, for more details, I invite you to read the sources.\nTechnical requests concerning the project Details of the context of realization : Details of the context of realization :.\nThe program must monitor SFTP user actions. These users have their HomeDir which are NFS mounts.\nThe user can upload anything to his account, but the program must detect video uploads (in some formats) and then perform a series of successive actions.\n\u0026lt;The operation should be as follows:\nFor each detected video repository, the video must check the allowed format, then it must be converted to .flv, have metadata, have a scree nshot (video thumbnail) and an email must be sent to the SFTP user\u0026rsquo;s email address.\nPreparation Several specific details are binding to begin with. The main one being the association of the account user\u0026rsquo;s mail to his own mail with the detection of the video repository.\nThat\u0026rsquo;s why I chose to start on a static file, knowing in advance the list of users.\n# Cr√©ation de la classe class Person: def __init__(self, name, surname, login, homedir, email, realpath): self.name = name self.surname = surname self.login = login self.homedir = homedir self.email = email self.realpath = realpath # # Et instanciation des utilisateurs # user_test = Person( 'Test', 'test', 'test', '/test/test/test.fr', 'test@test.fr', '/test.fr/') Then I worked on the core of the program. First of all to get the information from my user file and to make a grouping by list.\n# D√©finition des listes user_list = [] user_path = [] user_mail = [] user_realpath = [] # On appelle le fichier utilisateur pour impl√©menter les listes ### LOGIN ### for users.obj in gc.get_objects(): if isinstance(users.obj, users.Person): user_list.append(users.obj.login) ### DOCUMENT_ROOT ### for users.obj in gc.get_objects(): if isinstance(users.obj, users.Person): user_path.append(users.obj.homedir ### EMAIL ### for users.obj in gc.get_objects(): if isinstance(users.obj, users.Person): user_mail.append(users.obj.email) ### REALPATH ### for users.obj in gc.get_objects(): if isinstance(users.obj, users.Person): user_realpath.append(users.obj.realpath) The definition of a first function whose purpose will be the associative call with an ID position system.\ndef owner_func(): for filename in multiple_file_types('*.avi', '*.mov', '*.mp4', '*.mpg', '*.wmv'): # On affiche l'utilisateur owner = pwd.getpwuid(os.stat(filename).st_uid).pw_name # On v√©rifie que ce dernier est bien pr√©sent dans notr liste if owner in user_list: print('The owner of file is : ' + owner) position = user_list.index(owner) print('Owner_login_path :' + user_path[position]) mail = user_mail[position] mailp = print(mail) realpath = user_realpath[position] return(realpath) else: print('The owner was not found.') Pyinotifier Introduction Pyinotifier has functions related to the creation and deletion of data (IN_CREATE and IN_DELETE).\nOnce the basic setup was done, I used the basic definition to implement with the owner_func() function.\nclass EventHandler(pyinotify.ProcessEvent): def process_IN_CREATE(self, event): print('\\n\\n===========================') print(time.strftime(\u0026quot;%d/%m/%Y %H:%M:%S\u0026quot;)) evp = print('Path complet :' + '\\'' + event.pathname + '\\'') evn = print('Objet cr√©e : ' + '\\'' + event.name + '\\'') Ouser = event.pathname.split('/')[2] print('Utilisateur : ' + Ouser) Oplace = [i for i,x in enumerate(user_list) if x == Ouser][0] Orelatif = user_realpath[Oplace] Omail = user_mail[Oplace] print('Chroot Sftp : ' + Orelatif) print('Mail : ' + Omail) command = 'convert.bash '+str(event.pathname) previous_size=0 upload_finished = False try: while True: time.sleep(1) size=os.stat(event.pathname).st_size print(previous_size) print(size) if size == previous_size: break else: previous_size = size except: return False print(command) os.rename(event.pathname, event.pathname.replace(\u0026quot; \u0026quot;, \u0026quot;_\u0026quot;)) neventpathname = event.pathname.replace(' ', '_') neventname = event.name.replace(' ', '_') print(neventpathname + neventname) p1 = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) print('convert.bash', neventpathname) print(p1.stdout.read()) p1.wait() p2 = subprocess.Popen(['generatemetadata.bash', neventpathname]) p2.wait() p3 = subprocess.Popen(['extractpng.sh', neventpathname]) p3.wait() if neventname.endswith('.html') : p5 = subprocess.Popen(['mail.bash', Omail, Orelatif, neventpathname]) print('MAIL SENT') print('===========================') def process_IN_DELETE(self, event): print('\\n\\n===========================') print(time.strftime(\u0026quot;%d/%m/%Y %H:%M:%S\u0026quot;) + \u0026quot; Deleting:\u0026quot;, event.pathname) print('===========================') Explanations :\nThe principle of Pyinotify is to create an automatic action at a given event. Let\u0026rsquo;s take the deposit of a video corresponding to the right format as the event.\nThe IN_CREATE function is then triggered and will send us first information including: the creation date, the full path of the event, the relative path (HomeDir of the SFTP user), the user, his email\u0026hellip;\nIn a second step we apply the conversion to the right format (with ffmpeg). However, we have to make sure that the video is complete and submitted. This corresponds to the block :\ncommand = 'convert.bash '+str(event.pathname) previous_size=0 upload_finished = False try: while True: time.sleep(1) size=os.stat(event.pathname).st_size print(previous_size) print(size) if size == previous_size: break else: previous_size = size except: return False print(command) Then we will be able to run the bash scripts in subprocess.\nInterests The interests are multiple!\nThe first one is obvious since it is now possible to perform a logging on a service that was not natively available. The second one is that it is possible to set up a logging and PID system very simply.\nnotifier.loop(daemonize=True, callback=on_loop_func, pid_file='logs/pyinotify.pid', stdout='logs/%s.log' % timestr) This opens a very interesting new door on event automation by a Python subroutine that would take the work done by the kernel out of the box.\nI was just thinking of making a kind of API calling this type of operation scalable on different environments.\nThis project being in my famous ToDoList would be very close to a Master-Slaving system with certainly an imitation of what already exists with Puppet \u0026hellip; but in Python.\nWait and see what will be done !\n","date":"2017-07-30T12:53:16Z","permalink":"https://lebaron.sh/p/event-watcher-manager-python3/","title":"Event Watcher Manager Python3"}]