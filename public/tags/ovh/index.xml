<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ovh on Le Baron de Charlus</title>
    <link>https://lebaron.sh/tags/ovh/</link>
    <description>Recent content in ovh on Le Baron de Charlus</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>© Le Baron de Charlus</copyright>
    <lastBuildDate>Mon, 20 Jul 2020 12:00:00 +0000</lastBuildDate><atom:link href="https://lebaron.sh/tags/ovh/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bring Your Own Image</title>
      <link>https://lebaron.sh/posts/bring-your-own-image/</link>
      <pubDate>Mon, 20 Jul 2020 12:00:00 +0000</pubDate>
      
      <guid>https://lebaron.sh/posts/bring-your-own-image/</guid>
      <description>When I was working at OVHCloud company, I&amp;rsquo;ve developed a feature called BYOI (Bring Your Own Image).
Bring Your Own Image technology allows you to boot any cloud (or not) images on a baremetal.
Even if today not all editors are ready to provide completely agnostic images (and I mean UEFI ready and/or legacy boot), by triturating (packer) a bit the whole, we can have something functional.
This mark the first step towards the Hybrid IAC.</description>
    </item>
    
    <item>
      <title>Freebsd Pci Disk Space</title>
      <link>https://lebaron.sh/posts/freebsd-pci-disk-space/</link>
      <pubDate>Sun, 04 Mar 2018 14:56:33 +0000</pubDate>
      
      <guid>https://lebaron.sh/posts/freebsd-pci-disk-space/</guid>
      <description>When you create an OVH Public Cloud instance under Freebsd with a certain amount of disk space, let’s say 50G, you will find that it is not applied on your partition.
First let&amp;rsquo;s look at what we have:
# gpart show =&amp;gt; 40 10239920 da0 GPT (50G) [CORRUPT] 40 1024 1 freebsd-boot (512K) 1064 984 - free - (492K) 2048 10235904 2 freebsd-zfs (4.9G) 10237952 2008 - free - (1.0M) We note that our volume da0 is tagged as CORRUPT.</description>
    </item>
    
    <item>
      <title>Ubuntu Vrack Ovh Fix</title>
      <link>https://lebaron.sh/posts/ubuntu-vrack-ovh-fix/</link>
      <pubDate>Sun, 04 Mar 2018 14:41:25 +0000</pubDate>
      
      <guid>https://lebaron.sh/posts/ubuntu-vrack-ovh-fix/</guid>
      <description>You may have noticed it, but when you populate a PCI OVH instance under Ubuntu by activating Vrack, your Vm does not have its private IP at boot time. So, yes, I don&amp;rsquo;t like Ubuntu, but sometimes you don&amp;rsquo;t have a choice.
Anyway, all this to say that we don&amp;rsquo;t have our private IP and it&amp;rsquo;s too sad. (RT)
The fix trick is stupid. Very stupid.
Add:
allow-hotplug ens4 iface ens4 inet dhcp In /etc/network/interface file.</description>
    </item>
    
    <item>
      <title>Dockerize Nova client</title>
      <link>https://lebaron.sh/posts/dockerise-nova/</link>
      <pubDate>Tue, 08 Aug 2017 13:52:07 +0000</pubDate>
      
      <guid>https://lebaron.sh/posts/dockerise-nova/</guid>
      <description>With OVH Public Cloud, it is possible for you to control your OpenStack instances directly from the Nova client.
It is rather practical to go through the command line rather than having to access the manager which is often, let us say it, slow.
But to avoid having to prepare each time a working environment compatible with Nova, I find more interesting to directly create a Docker image for this purpose.</description>
    </item>
    
    <item>
      <title>Setup Rancher cluster on OVH Public Cloud</title>
      <link>https://lebaron.sh/posts/cluster-rancher-public-cloud-ovh/</link>
      <pubDate>Fri, 04 Aug 2017 19:04:11 +0000</pubDate>
      
      <guid>https://lebaron.sh/posts/cluster-rancher-public-cloud-ovh/</guid>
      <description>The world of hosting is changing, and so is the world of application development. Today we are turning less and less to dedicated hosting for a single application, but more to build the infrastructure that will support it.
In this sense, we prefer to use an Iaas solution on dedicated &amp;ldquo;bare metal&amp;rdquo; for our application overlay than pure &amp;ldquo;bare metal&amp;rdquo; per service.
Application deployment missions pushed by developers must fit with the technology and logic of production.</description>
    </item>
    
    <item>
      <title>Additional Volume Public Cloud Ovh</title>
      <link>https://lebaron.sh/posts/additional-volume-pci/</link>
      <pubDate>Mon, 31 Jul 2017 11:38:18 +0000</pubDate>
      
      <guid>https://lebaron.sh/posts/additional-volume-pci/</guid>
      <description>To add an additional disk/volume on your OVH public cloud, you need to follow some steps.
First, identify your new disk :
fdisk -l You can have different output depending of your system (sd{x}, vd{x}).
Then create a new partition :
# parted /dev/{{disk}} mktable gpt mkpart primary ext4 512 100% quit Format it :
mkfs -t ext4 -L rootfs /dev/{{disk}}1 Mount :
mount /dev/{{disk}}1 /mnt Let’s make it peristent, we need UUID.</description>
    </item>
    
    <item>
      <title>Additional Volume Public Cloud Ovh</title>
      <link>https://lebaron.sh/projects/hello/</link>
      <pubDate>Mon, 31 Jul 2017 11:38:18 +0000</pubDate>
      
      <guid>https://lebaron.sh/projects/hello/</guid>
      <description>To add an additional disk/volume on your OVH public cloud, you need to follow some steps.
First, identify your new disk :
fdisk -l You can have different output depending of your system (sd{x}, vd{x}).
Then create a new partition :
# parted /dev/{{disk}} mktable gpt mkpart primary ext4 512 100% quit Format it :
mkfs -t ext4 -L rootfs /dev/{{disk}}1 Mount :
mount /dev/{{disk}}1 /mnt Let’s make it peristent, we need UUID.</description>
    </item>
    
  </channel>
</rss>
